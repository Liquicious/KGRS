parser.add_argument('--dataset', type=str, default='3', help='which dataset to use')
parser.add_argument('--aggregator', type=str, default='sum', help='which aggregator to use')
parser.add_argument('--n_epochs', type=int, default=5, help='the number of epochs')
parser.add_argument('--neighbor_sample_size', type=int, default=4, help='the number of neighbors to be sampled')
parser.add_argument('--dim', type=int, default=32, help='dimension of user and entity embeddings')
parser.add_argument('--n_iter', type=int, default=2, help='number of iterations when computing entity representation')
parser.add_argument('--batch_size', type=int, default=65536, help='batch size')
parser.add_argument('--l2_weight', type=float, default=1e-7, help='weight of l2 regularization')
parser.add_argument('--lr', type=float, default=2e-2, help='learning rate')
parser.add_argument('--ratio', type=float, default=1, help='size of training dataset')


n users: 138159
n items: 16954
n entities: 102569
n relations: 32
ratings: 13.5m

epoch 0    train auc: 0.9733  f1: 0.9201    eval auc: 0.9686  f1: 0.9152    test auc: 0.9687  f1: 0.9155
epoch 1    train auc: 0.9808  f1: 0.9341    eval auc: 0.9731  f1: 0.9234    test auc: 0.9732  f1: 0.9239
epoch 2    train auc: 0.9874  f1: 0.9488    eval auc: 0.9769  f1: 0.9307    test auc: 0.9770  f1: 0.9309
epoch 3    train auc: 0.9910  f1: 0.9572    eval auc: 0.9775  f1: 0.9318    test auc: 0.9776  f1: 0.9321
epoch 4    train auc: 0.9931  f1: 0.9634    eval auc: 0.9770  f1: 0.9307    test auc: 0.9772  f1: 0.9312
epoch 5    train auc: 0.9944  f1: 0.9675    eval auc: 0.9765  f1: 0.9303    test auc: 0.9766  f1: 0.9306
epoch 6    train auc: 0.9953  f1: 0.9707    eval auc: 0.9759  f1: 0.9295    test auc: 0.9760  f1: 0.9299
epoch 7    train auc: 0.9959  f1: 0.9730    eval auc: 0.9756  f1: 0.9288    test auc: 0.9756  f1: 0.9292
