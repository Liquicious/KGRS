parser.add_argument('--dataset', type=str, default='3', help='which dataset to use')
parser.add_argument('--aggregator', type=str, default='sum', help='which aggregator to use')
parser.add_argument('--n_epochs', type=int, default=5, help='the number of epochs')
parser.add_argument('--neighbor_sample_size', type=int, default=4, help='the number of neighbors to be sampled')
parser.add_argument('--dim', type=int, default=16, help='dimension of user and entity embeddings')
parser.add_argument('--n_iter', type=int, default=2, help='number of iterations when computing entity representation')
parser.add_argument('--batch_size', type=int, default=65536, help='batch size')
parser.add_argument('--l2_weight', type=float, default=1e-7, help='weight of l2 regularization')
parser.add_argument('--lr', type=float, default=2e-2, help='learning rate')
parser.add_argument('--ratio', type=float, default=1, help='size of training dataset')


n users: 138493
n items: 6423
n entities: 115265
n relations: 7


epoch 0    train auc: 0.8101  f1: 0.7300    eval auc: 0.7823  f1: 0.7073    test auc: 0.7821  f1: 0.7069
epoch 1    train auc: 0.8365  f1: 0.7507    eval auc: 0.7989  f1: 0.7196    test auc: 0.7987  f1: 0.7194
epoch 2    train auc: 0.8523  f1: 0.7649    eval auc: 0.8012  f1: 0.7216    test auc: 0.8012  f1: 0.7215
epoch 3    train auc: 0.8649  f1: 0.7781    eval auc: 0.8004  f1: 0.7227    test auc: 0.8006  f1: 0.7231
epoch 4    train auc: 0.8761  f1: 0.7882    eval auc: 0.7981  f1: 0.7204    test auc: 0.7981  f1: 0.7205