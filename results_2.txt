parser.add_argument('--dataset', type=str, default='2', help='which dataset to use')
parser.add_argument('--aggregator', type=str, default='sum', help='which aggregator to use')
parser.add_argument('--n_epochs', type=int, default=5, help='the number of epochs')
parser.add_argument('--neighbor_sample_size', type=int, default=4, help='the number of neighbors to be sampled')
parser.add_argument('--dim', type=int, default=32, help='dimension of user and entity embeddings')
parser.add_argument('--n_iter', type=int, default=2, help='number of iterations when computing entity representation')
parser.add_argument('--batch_size', type=int, default=65536, help='batch size')
parser.add_argument('--l2_weight', type=float, default=1e-7, help='weight of l2 regularization')
parser.add_argument('--lr', type=float, default=2e-2, help='learning rate')
parser.add_argument('--ratio', type=float, default=1, help='size of training dataset')


n users: 138483
n items: 3458
n entities: 17724
n relations: 13
ratings: 9.7 mil

epoch 0    train auc: 0.8204  f1: 0.7165    eval auc: 0.7867  f1: 0.6863    test auc: 0.7872  f1: 0.6870
epoch 1    train auc: 0.8458  f1: 0.7421    eval auc: 0.7965  f1: 0.6978    test auc: 0.7970  f1: 0.6985
epoch 2    train auc: 0.8689  f1: 0.7648    eval auc: 0.7973  f1: 0.6978    test auc: 0.7976  f1: 0.6982
epoch 3    train auc: 0.8927  f1: 0.7914    eval auc: 0.7927  f1: 0.6952    test auc: 0.7932  f1: 0.6956
epoch 4    train auc: 0.9127  f1: 0.8141    eval auc: 0.7857  f1: 0.6886    test auc: 0.7864  f1: 0.6891
epoch 5    train auc: 0.9277  f1: 0.8340    eval auc: 0.7788  f1: 0.6844    test auc: 0.7793  f1: 0.6850
